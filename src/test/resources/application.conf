#spark settings
spark_ui_port = 4040

#Â storage params
s3_ssl_enabled = false
s3_endpoint = "http://127.0.0.1"
s3_path_style_access = true
checkpoint_path = /spark_checkpoint

# read from environment vars
aws_access_key_id = ""
aws_secret_access_key = ""

# locations
bucket_name = "METADATA"
bucket_landing_path = /landing
bucket_staging_path = /staging

##############
# Query Exec #
##############
# TODO  unit test the cache and ensure we got correct results, and cached is recomputed after X amount of time
# cache settings
cache_dataframes = false
clean_past_cache_delay = 2 minute
cache_expiry = 60 seconds
spark_sql_print_explain = false

############
# Pipeline #
############

# pipeline settings
trigger_time = 10 seconds
# this is the maxOpIndex number for an interval
compaction_record_interval = 10

# kafka settings
kafka_bootstrap_servers = "localhost:9092"
kafka_topic = "backbeat"

#############
# Compactor #
#############

# compaction
landing_purge_tolerance = 4 seconds


# graphite params
graphite {
  hostname = "" // turns search metrics off
  port = 2003
}
